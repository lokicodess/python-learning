

Two types of language models

1. Base LLM
    - predict the next word , based on trained text data

2. Instruction tuned LLM
    - Trained on huge base LLM's data but further tuned for input/output
      RLHF - Reinforcement learning with human feedback
      less likely to be toxic 

Principles for the LLM

1.  Clear and Specific Instruction
    - clear prompts != short prompts, longer prompts sometimes give more clarity
    - Use delimiters like "",`` etc 
    - Ask for structured output HTML,JSON
    - Check whether the conditions are satisfied, check assumption required to do the task
    - Few shot prompting give successful examples of completing task then ask model to perform the task

2.  Give the time to model to think
    - Basically if we are performing any task that is complex then we should give model a time to think otherwise
      it would make a guess that may be incorrect.
    - Specify the steps to complete a task
    - Instruct the model to work on its own solution before rushing to a conclusion 

Iterative Prompt Development

  - Be clear and specific
  - Analyze why result does not give desired output
  - Refine the idea and the prompt
  - Repeat

Inferring 
  - It means that the piece of information that we want to extract is not there in the data so our LLM  figure
    out how to extract it. 
  - For example: sentiments and emotions like anger , positive , negative , happy , sad etc
  - extract product and company name 

Transforming
  - Can use it translate between to one language to another
  - Convert json to html etc
  - Grammer checking
                

Expanding
  - Adding more context, instruction or example to generate more accurate result
  - less the degree(temperature) more the predictable response
  - for more creativity , variety use the higher temperature


  

  
